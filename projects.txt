Project was monolithic complex service dealing with multiple entities
- Created as restful service in Jax-RS (RestEasy) with in-house build discovery service
- Deployment on Wildfly (Jboss) with mysql, netezza and couchbase as backend
- Use design patterns like Singleton, CQRS (save in table & use couchbase)
- Couchbase data then used by backend services as a part of CQRS pattern
- Hourly job to replicate deltas from mysql to vertica
- Backend services like kafka/strom topology(bolt & spout) use couchbase and vertica for further processing
- Monolithic app connecting to multiple db (mysql, vertica) for reporting purposes
- template will be created by UI/Backend and Scheduler cron job will pickup and execute template (dynamic queries based on template)
- This was on-prem datacenter
- Artifacts deployed as war to jboss
- Jenkins used as CI/CD pipeline and SVN was version system
- on-premise private repository for artifacts (NO Images yet)
Move to Galera
- Decided to use galera for all instances
- Application is migrated and adjusted for log key
- Through & careful changes and testing
Move to Docker
- Started moving to docker image base
- Decided to move to spring boot micro services based
- Spring cloud config use to read config from gitlab service
- introduced kafka for circuit breaker and saga pattern and outbox pattern
- created multiple microservices and moved to spring from jax-rs world
- Creation of image and push to private repo
- deployment started on Portainer (Docker Swarm)
Migration to Cloud
- Creation of wrapper for couchbase and move to BigTable decided
- Started using helm
- Moved to gitlab and all CI/CD pipeline from jenkins
- Migration and through testing of the deployment files for kubernetes
- Deployment of services on kubernetes instead of portainer
- Stopped using gitlab config service instead started using Kubernetes ConfigMap and Secret
- Airflow DAGs creation
- Learn Spring boot, kafka, git, docker, kubernetes, ci/cd jenkins/gitlab, portainer, airflow, GCP

Saga Pattern:
1. Post Request to data asset for creation
   a. Data asset 
   b. synchrouch call using circuit breaker, eureka server to delivery platform, advertiser to check valid partner, if not error
   c. id created --> return success with data asset id and status (WIP)
   d. saga id (started) outbox started
   e. Scheduler Job pick up outbox table --> Started status --> send kafka message to create campaign service
   f. Create campaign add campaign and insert into it's own outbox with status (campaign_created) and saga id (campaign_created)
   g. if failed outbox status will be (campaign_failure) and saga (campaign_failure)
   h. campaign service scheduler picks (campaign_creating & failure) and send one by one - and update status (failure_sent)
   
Circuit Breaker:


1. Monolithic app in jax-rs
2. migration to log key to key
3. migration to G
4. smart file - kafka message to app
5. app read file load data in g
6. microservices
7. docker swarm (jenkins) - own repo
8. migrate from smart file to s3
9. migrate to k8s from swarm 
10. migrate to gitlab from private repo
11. gitlab CI/CD instead of jenkins
11. migrate from s3 to gcs buckets
12. migrate from cb to big table
13. create wrapper to work on both cb and big table


1. Logkey galera
2. ⁠*own caching mechanism for widgets then CB then redis*
3. ⁠surf framework (eureka type server)
4. ⁠rpm, ear/war files
5. ⁠*couchbase wrapper to easily switch to big table*
6. ⁠snowflake jpa issues dialect vertica
7. ⁠monolithic to micro service
8. ⁠*working with security team - image vulnerable using grype or trivy*
9. ⁠iap issue with services
10. ⁠liveness/readiness creates issues 
11. ⁠migration of couchbase data to db
12. ⁠kafka lag issue with storm topology
13. ⁠airflow composer
14. ⁠gcp bucket or s3 bucket working
15. ⁠working closely with techops/netops help understand system 
16. ⁠akamai, catch point, datadog, splunk
